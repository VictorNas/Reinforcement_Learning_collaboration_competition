{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad21cf0",
   "metadata": {},
   "source": [
    "### 1. The Enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca427d7",
   "metadata": {},
   "source": [
    "In this project we use the  MADDPG algorithm introduced in the paper [Multi-Agent Actor-Critic for Mixed\n",
    "Cooperative-Competitive Environments](https://arxiv.org/pdf/1706.02275.pdf) to solve the [Tennis Enviroment](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#tennis). \n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation.  Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "The task is episodic, and in order to solve the environment, your agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically,\n",
    "\n",
    "- After each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores.\n",
    "- This yields a single **score** for each episode.\n",
    "\n",
    "The environment is considered solved, when the average (over 100 episodes) of those **scores** is at least +0.5.\n",
    "\n",
    "All the implemantation is avaliable in the [Training Notebook](Tennis_Training.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f592df44",
   "metadata": {},
   "source": [
    "### 2. Learning Algorithm :  Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments (MADDPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e865773d",
   "metadata": {},
   "source": [
    "#### The Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d322c4fa",
   "metadata": {},
   "source": [
    "![alt text](assets/MADDPG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27534992",
   "metadata": {},
   "source": [
    "*Image from: [Multi-Agent Actor-Critic for Mixed\n",
    "Cooperative-Competitive Environments](https://arxiv.org/pdf/1706.02275.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2db7be",
   "metadata": {},
   "source": [
    "MADDPG was introduced in the paper [Multi-Agent Actor-Critic for Mixed\n",
    "Cooperative-Competitive Environments](https://arxiv.org/pdf/1706.02275.pdf) and was described by the actors as an adaptation of actor-critic methods that considers action policies\n",
    "of other agents and is able to successfully learn policies that require complex multiagent coordination.\n",
    "\n",
    "It was proposed to serve as general-purpose multi-agent learning\n",
    "algorithm that could be applied not just to cooperative games with explicit communication channels,\n",
    "but competitive games and games involving only physical interactions between agents.\n",
    "\n",
    "MADDPG accomplish its goal by adopting the framework of centralized training with\n",
    "decentralized execution. Thus, it allows the policies to use extra information to ease training, so\n",
    "long as this information is not used at test time.\n",
    "\n",
    "In short, there are N agents so will be N policies to learn. Each actor will try to estimate the best action given the agent observation. Each critic will receive the actions of all agents as long with all the states observations of all agents and will try to estimate the action value function for its agent.\n",
    "\n",
    "The experience replay buffer contains the tuples (x, x,0, a1, . . . , aN , r1, . . . , rN ), recording\n",
    "experiences of all agents.\n",
    "\n",
    "MADDPG also uses target networks to reduce instability during the training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8524243",
   "metadata": {},
   "source": [
    "#### Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b42e28",
   "metadata": {},
   "source": [
    "![alt text](assets/MADDPGcode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caed0e1",
   "metadata": {},
   "source": [
    "*Image from: [Multi-Agent Actor-Critic for Mixed\n",
    "Cooperative-Competitive Environments](https://arxiv.org/pdf/1706.02275.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a250a9ca",
   "metadata": {},
   "source": [
    "#### Adaptation to solve the Tennis Enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c342f6bc",
   "metadata": {},
   "source": [
    "The code implementation in the project is an adaptation of the DDPG code used to solve the [Reacher environment](https://github.com/VictorNas/DDPG-Continuous-Control/blob/main/continuous-control-multiple/ddpg_agent.py).\n",
    "\n",
    "To solve the enviroment two critics networks and two actors networks were trained (one for each agent). During training, each critic receives the observations of both agents along with the actions given by the actors and try to estimate the action value function of its actor. During inference, only the actors are used.\n",
    "\n",
    "At first , to balance exploration and explotation was used **Ornstein-Uhlenbeck process** but the agent was not able to solve the enviroment with that. We are able to solve the enviroment using the **eps greedy** method.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1b9b0",
   "metadata": {},
   "source": [
    "#### Actors Archtecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e573c2e4",
   "metadata": {},
   "source": [
    "Each **Actor** has the following archtecture:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "897a8fb0",
   "metadata": {},
   "source": [
    "Actor(\n",
    "  (fc1): Linear(in_features=24, out_features=128, bias=True)\n",
    "  (fc2): Linear(in_features=128, out_features=256, bias=True)\n",
    "  (fc3): Linear(in_features=256, out_features=2, bias=True)\n",
    "  (batch_fc1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb3d8a1",
   "metadata": {},
   "source": [
    "* The input of the network is the observation vector of its agent.\n",
    "* The First linear layer **fc1** is follewed by a BatchNorm Layer  **batch_fc1** and a **LeakyRelu** activation function.\n",
    "\n",
    "* The Second linear layer **fc2** is follewed by **LeakyRelu** activation function.\n",
    "\n",
    "* The Third linear layer **fc3** is follewed by **tanh** activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e7e8a5",
   "metadata": {},
   "source": [
    "#### Critic Archtecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40897c3a",
   "metadata": {},
   "source": [
    "Each **Critic** Network has the following archtecture"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18c36455",
   "metadata": {},
   "source": [
    "Critic(\n",
    "  (fcs1): Linear(in_features=48, out_features=128, bias=True)\n",
    "  (fc2): Linear(in_features=132, out_features=256, bias=True)\n",
    "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
    "  (batch_fc1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb1379",
   "metadata": {},
   "source": [
    "* The input of the network is the observation vector of coth agents cooncatenated. Since each observation vector has size of 24, the input size is 48.\n",
    "\n",
    "* The First linear layer **fc1** is follewed by a BatchNorm Layer  **batch_fc1** and a **LeakyRelu** activation function.\n",
    "\n",
    "* The output of the LeakyRelu is concatenated with the actions of both agents.\n",
    "\n",
    "* The Second linear layer **fc2** is follewed by **LeakyRelu** activation function.\n",
    "\n",
    "* The Third linear layer **fc3** is not follewed by any activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6e6b29",
   "metadata": {},
   "source": [
    "#### Chosen Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae9516",
   "metadata": {},
   "source": [
    "After some experimentation the following hyperparameters were choosen:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7803b2",
   "metadata": {},
   "source": [
    "| hyperparameter | Value | Descrption\n",
    "| --- | --- | --- |\n",
    "| BUFFER_SIZE | 1e6|replay buffer size|\n",
    "| BATCH_SIZE | 128|minibatch size|\n",
    "| GAMMA | 0.99 |discount factor|\n",
    "| TAU | 1e-3|for soft update of target parameters|\n",
    "|LR_ACTOR| 3e-4|learning rate of the actor |\n",
    "|LR_CRITIC|1e-3 |learning rate of the critic|\n",
    "|UPDATE_STEPS|10 |update every steps|\n",
    "|NUMBER_UPDATES|8|number of times we update the network after each UPDATE_STEPS|\n",
    "|OPTMIZER_ACTOR|ADAM|Optimizer of the actor|\n",
    "|OPTMIZER_CRITIC|ADAM|Optimizer of the critic|\n",
    "|eps_init|1.0|Initial value of eps ofr the eps Greedy method|\n",
    "|eps_min|0.01|Minimal value of eps ofr the eps Greedy method|\n",
    "|eps_decay|0.95|Decay of the eps at each episode|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd390f",
   "metadata": {},
   "source": [
    "### 2.Plot of Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d45da7a",
   "metadata": {},
   "source": [
    "The environment is considered solved, when the average (over 100 episodes) of those **scores** is at least +0.5.\n",
    "The agent was trained for almost 3600 episodes and was able to solve the enviroment at first in episode 2417.\n",
    "Near the episode 3000 the agent as able to achieve a mean score of 0.5 over the last 100 consecutives episodes.\n",
    "Near the episode 3500 the agent was able to achieve a mean score of 1.51 over the last 100 consecutives episodes.\n",
    "\n",
    "OBS: The red lines represents the goal score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d46ff",
   "metadata": {},
   "source": [
    "![alt text](assets/scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba4a52",
   "metadata": {},
   "source": [
    "### Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa327c3",
   "metadata": {},
   "source": [
    "There are some imporvements that may be worth to be done in order to make the agent to solve the enviroment faster.\n",
    "1. Try to implement other algorithms like Trust Region Policy Optimization (TRPO)and Proximal Policy Optimization (PPO).\n",
    "2. Try to implement the Prioritized Buffer Replay."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
